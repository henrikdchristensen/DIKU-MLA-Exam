{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The basic assumptions (5 points) [Yevgeny]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Throughout the course we have assumed that data samples are independent. Explain why this assumption is important. What breaks if it does not hold and how it breaks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Throughout the course we have assumed that new data points are sampled from the same distribution as the data points in the training set. Explain why this assumption is important. What breaks down if it does not hold and how it breaks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. M companies (15 points) [Yevgeny]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M companies have independently collected data and built binary classifiers for a problem that is of interest to you. You have bought both their data and the classifiers (a single classifier from each company), and now you want to select a classifier that you will use.\n",
    "\n",
    "Let $i \\in \\{1, ..., M\\}$ index the companies, let $S_i$ denote the data set collected by company $i$, let $n_i = |S_i|$ be the size of $S_i$, and let $\\hat{h}^\\ast_i$ be the classifier produced by company $i$ by using the data set $S_i$. Let $\\delta \\in (0, 1)$ be a confidence parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Derive a generalization bound (a bound on $L(\\hat{h}^\\ast_i$)) that would hold for all the prediction rules with probability at least $1 − \\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain how you would use the bound to select a prediction rule to use on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: you are allowed to introduce new notation, which should be clearly defined in terms of existing notation. For example, you can define $n = \\sum_{i=1}^M n_i$ or $S = \\cup_{i=1}^M S_i$. If you introduce convenient notation, you may be able to simplify your life (in the sense that you will need to write less). If you do not manage to come up with convenient notation, you may need to write a bit more. Your answer must be supported by a complete derivation or references to the relevant theorems in the course material. In case you use theorems from the course material, you must verify that their assumptions hold. You are not allowed to reference external material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Learning functions on a triangle by discretization (15 points) [Yevgeny]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to learn an arbitrary binary function on a triangle by discretizing the triangle into smaller triangles and assigning binary labels to the subtriangles. The partition at level $d$ is constructed by recursively splitting each triangle at level $d − 1$ into four subtriangles, as illustrated in Figure 1. We get a sample $S$ of size $n$ to learn the function. Let $\\mathcal{H}_d$ be the hypothesis set of all possible label assignments to a partition at level $d$ (in Figure 1 we provide an illustration of the first three levels). Let $\\mathcal{H} = \\cup_{d=1}^\\infty \\mathcal{H}_d$ be the hypothesis set of all possible prediction rules that we consider. For a prediction rule $h$ let $d(h)$ denote the corresponding partition level, so that $h \\in \\mathcal{H}_{d(h)}$.\n",
    "\n",
    "![fig1](images/figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Derive a generalization bound for learning with $\\mathcal{H}_d$. (I.e., a bound on $L(h)$ that holds for all $h \\in \\mathcal{H}_d$ with probability at least $1 − \\delta$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Derive a generalization bound for learning with $\\mathcal{H}$. (I.e., a bound on $L(h)$ that holds for all $h \\in \\mathcal{H}$ with probability at least $1 − \\delta$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain how to use the latter bound to select a prediction rule $h \\in \\mathcal{H}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the maximal partition level $d$ for which your bound is non-vacuous?\n",
    "(It is sufficient to give an order of magnitude, you do not need to make the\n",
    "precise calculation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Let $\\hat{h}^\\ast_d$ be a prediction rule at level $d$ that achieves the minimal empirical error. Consider the bound on $L(\\hat{h}^\\ast_d)$ you derived earlier in Point 2 as a function of $d$. Explain which terms in the bound on $L(\\hat{h}^\\ast_d)$ (if any) increase with $d$ and which terms in the bound on $L(\\hat{h}^\\ast_d)$ (if any) decrease with $d$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ensemble methods and random forests (20 points) [Christian]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Depth of binary decision trees\n",
    "Assume a training data $S$ set with n training data points. If all of these data point end up in a different leaf, what is the minimum and maximum depth of a binary decision tree build on $S$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Bias-variance decomposition\n",
    "In the lecture on decision trees and random forests, it is claimed on the slide \"Bias-variance decomposition, noiseless case\" that\n",
    "$$\n",
    "2\\mathbb{E}_S \\mathbb{E}_p\\{(y − \\mathbb{E}_{S'}\\{h_{S'}(x)\\})(\\mathbb{E}_{S'}\\{h_{S'}(x)\\} − h_S(x))\\}\n",
    "$$\n",
    "vanishes. Prove – using the notation of the lecture slides – that this is indeed true. The proof is very short, but you have to provide the intermediate steps and justify why each step is correct.\n",
    "\n",
    "Note: In the proof, you need to \"move expectations around\". If you do so, state why this is valid. You can assume that the order of expectations can be changed in the sense that the conditions of Fubini’s theorem are fulfilled and you may change the order of double integrals. If the last sentence confuses you, ignore it and simply assume that all expectations are over discrete random variables taking only finite values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Gain\n",
    "The lecture on decision trees and random forests introduced the information gain\n",
    "$$\n",
    "G_{d,\\theta}(S) = Q(S) − \\frac{|L_{d,\\theta}|}{|S|} Q(L_{d,\\theta}) − \\frac{|R_{d,\\theta}|}{|S|} Q(R_{d,\\theta}),\n",
    "$$\n",
    "where $Q$ is some impurity measure. Let’s consider a regression task and training data $S = \\{(x_1, y_1), ..., (x_n, y_n)\\}$ with $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$ for $i = 1, ..., n$. Let the impurity measure be the mean squared error. That is, for A ⊆ S we have\n",
    "$$\n",
    "Q(A) = \\min_{c\\in\\mathbb{R}} \\frac{1}{|A|} \\sum_{(x,y)\\in A} (y − c)^2.\n",
    "$$\n",
    "Proof that $G_{d,\\theta}(S) \\ge 0$ for any choice of $d$ and $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Convolutional Neural Networks (7 points) [Christian]\n",
    "Consider two-dimensional input data with C channels. The number of channels is fixed, however, the spatial size $r \\times s$ may vary. For example, think of color images of different sizes $r \\times s$, which would all have $C = 3$ but different numbers of pixels.\n",
    "\n",
    "Now you would like to implement binary logistic regression on top of these data. The classifier should produce a classification in {0, 1} for each of the r×s spatial positions, and each of these predictions should only depend on the $C$ channel values at this position (i.e., there is no spatial interaction). The classifier should be the same for each spatial position, that is, its weights/parameters should not depend on the spatial position $r \\times s$.\n",
    "\n",
    "In standard deep learning frameworks, this classification can be implemented by a single convolutional layer.\n",
    "\n",
    "Please explain in one to three sentences how this can be done. If you show the corresponding PyTorch line, which is not required but may be helpful, you still need to explain what the code is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. National Forest Inventory (18 points) [Christian]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantification of forest biomass stocks and their dynamics is important for implementing effective climate change mitigation measures by aiding local forest management, studying processes driving af-, re-, and deforestation, and improving the accuracy of carbon accounting. Remote sensing using airborne LiDAR can be used to perform measurements of vegetation structure at large scale. The state-of-the-art for predicting forest biomass from LiDAR point clouds is to voxelize the 3D data and compute summarizing statistics of the point distribution along the vertical axis, such as mean heights, relative height quantiles, or metrics of heterogeneity. These simple statistical features, potentially combined with other features, then serve as inputs to prediction models.\n",
    "\n",
    "The Danish National Forest Inventory (NFI) gathers information about Danish forests and computes statistics for the formation and assessment of national or regional forest programs, sustainability assessments, investment calculations for forest industry, strategic-level planning in general, and reporting to international conventions. The Danish NFI is based on a grid covering the entire land surface of the country (Nord-Larsen and Johannsen, 2016). The measurements are done at the level of subplots, where each subplots has a radius of 15 m.\n",
    "\n",
    "We link the data from the Danish NFI with LiDAR data similar to Oehmcke et al. (2022), who propose a deep learning approach directly operating on the 3D point clouds. The main task is to predict aboveground tree biomass within a subplot from statistics of the 3D LiDAR data of the subplot. The data in this assignment were taken from 2013 to 2017 and were heavily preprocessed and cleaned for this assignment. First, features were derived from the point clouds of the single plots. Second, only plots with non-zero biomass and trees that are either all broadleaf trees or all conifer trees were selected. Finally, outliers were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filename = 'NFI_filtered_cleaned.csv'\n",
    "filepath = os.path.join('data', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "url_data = 'https://raw.githubusercontent.com/christian-igel/ML/main/data/' + filename\n",
    "\n",
    "if not os.access(filepath, os.R_OK):\n",
    "    print('downloading', url_data)\n",
    "    try:\n",
    "        r = requests.get(url_data)\n",
    "        r.raise_for_status()\n",
    "        open(filepath, 'wb').write(r.content)\n",
    "    except Exception as e:\n",
    "        print('download failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h_mean_1_</th>\n",
       "      <th>h_mean_2_</th>\n",
       "      <th>h_std_1_</th>\n",
       "      <th>h_std_2_</th>\n",
       "      <th>h_coov_1_</th>\n",
       "      <th>h_coov_2_</th>\n",
       "      <th>h_kur_1_</th>\n",
       "      <th>h_kur_2_</th>\n",
       "      <th>h_skew_1_</th>\n",
       "      <th>h_skew_2_</th>\n",
       "      <th>...</th>\n",
       "      <th>red_q50</th>\n",
       "      <th>red_q25</th>\n",
       "      <th>blue_q75</th>\n",
       "      <th>blue_q50</th>\n",
       "      <th>blue_q25</th>\n",
       "      <th>green_q75</th>\n",
       "      <th>green_q50</th>\n",
       "      <th>green_q25</th>\n",
       "      <th>BMag_ha</th>\n",
       "      <th>C_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "      <td>3156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.409585</td>\n",
       "      <td>10.662752</td>\n",
       "      <td>5.105565</td>\n",
       "      <td>3.577681</td>\n",
       "      <td>1.051682</td>\n",
       "      <td>0.368356</td>\n",
       "      <td>1.838990</td>\n",
       "      <td>0.649883</td>\n",
       "      <td>0.288754</td>\n",
       "      <td>-0.315015</td>\n",
       "      <td>...</td>\n",
       "      <td>22139.944233</td>\n",
       "      <td>18636.451204</td>\n",
       "      <td>22296.699620</td>\n",
       "      <td>19473.237009</td>\n",
       "      <td>16995.629911</td>\n",
       "      <td>25513.125475</td>\n",
       "      <td>21920.365019</td>\n",
       "      <td>18655.026616</td>\n",
       "      <td>113.640865</td>\n",
       "      <td>0.468314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.411430</td>\n",
       "      <td>5.781779</td>\n",
       "      <td>2.692018</td>\n",
       "      <td>1.783427</td>\n",
       "      <td>0.852839</td>\n",
       "      <td>0.140408</td>\n",
       "      <td>17.111033</td>\n",
       "      <td>4.515893</td>\n",
       "      <td>1.561340</td>\n",
       "      <td>0.909943</td>\n",
       "      <td>...</td>\n",
       "      <td>8326.425086</td>\n",
       "      <td>8049.423340</td>\n",
       "      <td>7008.356714</td>\n",
       "      <td>6988.936708</td>\n",
       "      <td>6880.897220</td>\n",
       "      <td>7293.151396</td>\n",
       "      <td>7354.904510</td>\n",
       "      <td>7232.853660</td>\n",
       "      <td>104.941748</td>\n",
       "      <td>0.499074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.007347</td>\n",
       "      <td>1.117059</td>\n",
       "      <td>0.071730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.926685</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-13.816676</td>\n",
       "      <td>-3.747779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.906589</td>\n",
       "      <td>5.994878</td>\n",
       "      <td>3.073276</td>\n",
       "      <td>2.330300</td>\n",
       "      <td>0.567644</td>\n",
       "      <td>0.272190</td>\n",
       "      <td>-1.277054</td>\n",
       "      <td>-0.690741</td>\n",
       "      <td>-0.627047</td>\n",
       "      <td>-0.845669</td>\n",
       "      <td>...</td>\n",
       "      <td>16384.000000</td>\n",
       "      <td>12800.000000</td>\n",
       "      <td>17600.000000</td>\n",
       "      <td>14848.000000</td>\n",
       "      <td>12544.000000</td>\n",
       "      <td>20224.000000</td>\n",
       "      <td>16640.000000</td>\n",
       "      <td>13568.000000</td>\n",
       "      <td>28.934917</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.438852</td>\n",
       "      <td>10.085121</td>\n",
       "      <td>4.823045</td>\n",
       "      <td>3.371879</td>\n",
       "      <td>0.835525</td>\n",
       "      <td>0.352517</td>\n",
       "      <td>-0.623700</td>\n",
       "      <td>-0.122410</td>\n",
       "      <td>0.095032</td>\n",
       "      <td>-0.389580</td>\n",
       "      <td>...</td>\n",
       "      <td>22016.000000</td>\n",
       "      <td>18432.000000</td>\n",
       "      <td>22016.000000</td>\n",
       "      <td>19200.000000</td>\n",
       "      <td>16640.000000</td>\n",
       "      <td>25344.000000</td>\n",
       "      <td>21760.000000</td>\n",
       "      <td>18688.000000</td>\n",
       "      <td>86.230206</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.894731</td>\n",
       "      <td>14.578052</td>\n",
       "      <td>6.953171</td>\n",
       "      <td>4.658597</td>\n",
       "      <td>1.272893</td>\n",
       "      <td>0.435651</td>\n",
       "      <td>1.014367</td>\n",
       "      <td>0.883007</td>\n",
       "      <td>0.947310</td>\n",
       "      <td>0.161931</td>\n",
       "      <td>...</td>\n",
       "      <td>28160.000000</td>\n",
       "      <td>24320.000000</td>\n",
       "      <td>26368.000000</td>\n",
       "      <td>23552.000000</td>\n",
       "      <td>20992.000000</td>\n",
       "      <td>30272.000000</td>\n",
       "      <td>26880.000000</td>\n",
       "      <td>23552.000000</td>\n",
       "      <td>169.278397</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31.577832</td>\n",
       "      <td>31.654262</td>\n",
       "      <td>14.545132</td>\n",
       "      <td>12.496709</td>\n",
       "      <td>13.132664</td>\n",
       "      <td>1.359926</td>\n",
       "      <td>573.404330</td>\n",
       "      <td>205.440694</td>\n",
       "      <td>18.965810</td>\n",
       "      <td>7.757708</td>\n",
       "      <td>...</td>\n",
       "      <td>51200.000000</td>\n",
       "      <td>51200.000000</td>\n",
       "      <td>53376.000000</td>\n",
       "      <td>50176.000000</td>\n",
       "      <td>50176.000000</td>\n",
       "      <td>53376.000000</td>\n",
       "      <td>47104.000000</td>\n",
       "      <td>44288.000000</td>\n",
       "      <td>956.210950</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         h_mean_1_    h_mean_2_     h_std_1_     h_std_2_    h_coov_1_  \\\n",
       "count  3156.000000  3156.000000  3156.000000  3156.000000  3156.000000   \n",
       "mean      7.409585    10.662752     5.105565     3.577681     1.051682   \n",
       "std       5.411430     5.781779     2.692018     1.783427     0.852839   \n",
       "min       0.007347     1.117059     0.071730     0.000000     0.075114   \n",
       "25%       2.906589     5.994878     3.073276     2.330300     0.567644   \n",
       "50%       6.438852    10.085121     4.823045     3.371879     0.835525   \n",
       "75%      10.894731    14.578052     6.953171     4.658597     1.272893   \n",
       "max      31.577832    31.654262    14.545132    12.496709    13.132664   \n",
       "\n",
       "         h_coov_2_     h_kur_1_     h_kur_2_    h_skew_1_    h_skew_2_  ...  \\\n",
       "count  3156.000000  3156.000000  3156.000000  3156.000000  3156.000000  ...   \n",
       "mean      0.368356     1.838990     0.649883     0.288754    -0.315015  ...   \n",
       "std       0.140408    17.111033     4.515893     1.561340     0.909943  ...   \n",
       "min       0.000000    -1.926685    -3.000000   -13.816676    -3.747779  ...   \n",
       "25%       0.272190    -1.277054    -0.690741    -0.627047    -0.845669  ...   \n",
       "50%       0.352517    -0.623700    -0.122410     0.095032    -0.389580  ...   \n",
       "75%       0.435651     1.014367     0.883007     0.947310     0.161931  ...   \n",
       "max       1.359926   573.404330   205.440694    18.965810     7.757708  ...   \n",
       "\n",
       "            red_q50       red_q25      blue_q75      blue_q50      blue_q25  \\\n",
       "count   3156.000000   3156.000000   3156.000000   3156.000000   3156.000000   \n",
       "mean   22139.944233  18636.451204  22296.699620  19473.237009  16995.629911   \n",
       "std     8326.425086   8049.423340   7008.356714   6988.936708   6880.897220   \n",
       "min        0.000000      0.000000    256.000000      0.000000      0.000000   \n",
       "25%    16384.000000  12800.000000  17600.000000  14848.000000  12544.000000   \n",
       "50%    22016.000000  18432.000000  22016.000000  19200.000000  16640.000000   \n",
       "75%    28160.000000  24320.000000  26368.000000  23552.000000  20992.000000   \n",
       "max    51200.000000  51200.000000  53376.000000  50176.000000  50176.000000   \n",
       "\n",
       "          green_q75     green_q50     green_q25      BMag_ha       C_frac  \n",
       "count   3156.000000   3156.000000   3156.000000  3156.000000  3156.000000  \n",
       "mean   25513.125475  21920.365019  18655.026616   113.640865     0.468314  \n",
       "std     7293.151396   7354.904510   7232.853660   104.941748     0.499074  \n",
       "min      256.000000      0.000000      0.000000     0.002059     0.000000  \n",
       "25%    20224.000000  16640.000000  13568.000000    28.934917     0.000000  \n",
       "50%    25344.000000  21760.000000  18688.000000    86.230206     0.000000  \n",
       "75%    30272.000000  26880.000000  23552.000000   169.278397     1.000000  \n",
       "max    53376.000000  47104.000000  44288.000000   956.210950     1.000000  \n",
       "\n",
       "[8 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "df.to_csv(filepath, index=False)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define relevant features. Do not use other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_target = 'BMag_ha'\n",
    "classification_target = 'C_frac'\n",
    "features = ['h_mean_1_', 'h_mean_2_', 'h_std_1_', 'h_std_2_', 'h_coov_1_', 'h_coov_2_', 'h_skew_1_', 'h_skew_2_',\n",
    "            'IR_', 'h_q5_1_', 'h_q10_1_', 'h_q25_1_', 'h_q50_1_', 'h_q75_1_', 'h_q90_1_', 'h_q95_1_', 'h_q99_1_', 'h_q5_2_',\n",
    "            'h_q10_2_', 'h_q25_2_', 'h_q50_2_', 'h_q75_2_', 'h_q90_2_', 'h_q95_2_', 'h_q99_2_', 'red_q75', 'red_q50', 'red_q25',\n",
    "            'blue_q75', 'blue_q50', 'blue_q25', 'green_q75', 'green_q50', 'green_q25']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes in a training, test, and validation split. Combine training- and validation set into a single data frame, for tasks that don't require a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set with train and validation data\n",
    "df_trainval = df.query(\"split in ['train', 'val']\")\n",
    "\n",
    "# Define set with test data\n",
    "df_test = df.query(\"split == 'test'\")\n",
    "\n",
    "# Select only the features we want to use\n",
    "X_trainval = df_trainval[features]\n",
    "X_test = df_test[features]\n",
    "\n",
    "# Convert target values from dataframes to numpy arrays\n",
    "y_trainval = df_trainval[[regression_target]].values.ravel()\n",
    "y_test = df_test[[regression_target]].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The property `C_frac` is not used as an input feature. It give the fraction of conifer trees in the subplot. The data are filtered so that `C_frac` is either zero or one. This feature can be used to classify the subplot into broadleaf or confier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to evaluate regression approaches on the data. The regression target is the aboveground biomass `BMag_ha`.\n",
    "\n",
    "Build the models using the training and validation data. The test data must only be used for final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Apply linear Regression without regularization. Report the test $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score (training): 0.712\n",
      "R2 score (test): 0.739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define linear regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Fit model on TrainVal data\n",
    "lr_model.fit(X_trainval, y_trainval)\n",
    "\n",
    "# Predict on TrainVal and Test data\n",
    "lr_trainval_y_pred = lr_model.predict(X_trainval)\n",
    "lr_test_y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Print R2 scores\n",
    "print(f\"R2 score (training): {r2_score(y_trainval, lr_trainval_y_pred):.3f}\")\n",
    "print(f\"R2 score (test): {r2_score(y_test, lr_test_y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply random forest with 100 and 1000 trees. Use the OOB score to determine which number of trees is better. Report OOB scores of both models and the test $R^2$ score of the better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "Best number of estimators: 1000\n",
      "OOB score: 0.698\n",
      "R2 score (training): 0.902\n",
      "R2 score (test): 0.731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define parameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 1000],\n",
    "}\n",
    "\n",
    "# Define random forest model\n",
    "rf_model = RandomForestRegressor(random_state=0, oob_score=True)\n",
    "\n",
    "# Grid search\n",
    "rf_grid_search = GridSearchCV(rf_model, param_grid=rf_param_grid, cv=3, n_jobs=-1, verbose=3)\n",
    "\n",
    "# Fit model with grid search on TrainVal data\n",
    "rf_grid_search.fit(X_trainval, y_trainval)\n",
    "\n",
    "# Best model from grid search\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Predict with best model on TrainVal and Test data\n",
    "rf_best_model_trainval_y_pred = rf_best_model.predict(X_trainval)\n",
    "rf_best_model_test_y_pred = rf_best_model.predict(X_test)\n",
    "\n",
    "# Print best number of estimators\n",
    "print(f\"Best number of estimators: {rf_grid_search.best_params_['n_estimators']}\")\n",
    "# Print OOB score\n",
    "print(f\"OOB score: {rf_best_model.oob_score_:.3f}\")\n",
    "# Print R2 scores\n",
    "print(f\"R2 score (training): {r2_score(y_trainval, rf_best_model_trainval_y_pred):.3f}\")\n",
    "print(f\"R2 score (test): {r2_score(y_test, rf_best_model_test_y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply $k$-nearest-neighbor regression. Find out if normalization to zero mean and unit variance helps. Describe how you did that. Use cross-validation to determine the number of neighbors from the range {1, 3, 5, ..., 47, 49}. Describe how you determined the number of neighbors. Report training and test $R^2$ score of the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of neighbors: 33\n",
      "R2 score (training): 0.135\n",
      "R2 score (test): 0.027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "# Define parameter grid\n",
    "knn_param_grid = {'n_neighbors': list(range(1, 50, 2))} # odd numbers from 1 to 50 (majority vote, no ties)\n",
    "\n",
    "# Define kNN model and grid search\n",
    "knn_grid_search = GridSearchCV(estimator=KNeighborsRegressor(), param_grid=knn_param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit model with grid search on training set\n",
    "knn_grid_search.fit(X_trainval, y_trainval)\n",
    "\n",
    "# Best model from grid search\n",
    "knn_best_unnormalized_model = knn_grid_search.best_estimator_\n",
    "\n",
    "# Predict with best model on TrainVal and Test data\n",
    "knn_best_unnormalized_model_trainval_y_pred = knn_best_unnormalized_model.predict(X_trainval)\n",
    "knn_best_unnormalized_model_test_y_pred = knn_best_unnormalized_model.predict(X_test)\n",
    "\n",
    "# Print best number of neighbors\n",
    "print(f\"Best number of neighbors: {knn_grid_search.best_params_['n_neighbors']}\")\n",
    "# Print R2 score for training set and test set\n",
    "print(f\"R2 score (training): {r2_score(y_trainval, knn_best_unnormalized_model_trainval_y_pred):.3f}\")\n",
    "print(f\"R2 score (test): {r2_score(y_test, knn_best_unnormalized_model_test_y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of neighbors: 41\n",
      "R2 score (training): 0.706\n",
      "R2 score (test): 0.732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming X_train and X_test are your training and test datasets\n",
    "scaler = StandardScaler()\n",
    "X_trainval_normalized = scaler.fit_transform(X_trainval)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Define parameter grid\n",
    "knn_param_grid = {'n_neighbors': list(range(1, 50, 2))}  # {1, 3, 5, ..., 49}\n",
    "\n",
    "# Define kNN model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Grid search\n",
    "knn_grid_search = GridSearchCV(estimator=knn_model, param_grid=knn_param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit model with grid search on training set\n",
    "knn_grid_search.fit(X_trainval_normalized, y_trainval)\n",
    "\n",
    "# Best model from grid search\n",
    "knn_best_normalized_model = knn_grid_search.best_estimator_\n",
    "\n",
    "# Predict with best model on TrainVal and Test data\n",
    "knn_best_normalized_model_trainval_y_pred = knn_best_normalized_model.predict(X_trainval_normalized)\n",
    "knn_best_normalized_model_test_y_pred = knn_best_normalized_model.predict(X_test_normalized)\n",
    "\n",
    "# Print best number of neighbors\n",
    "print(f\"Best number of neighbors: {knn_grid_search.best_params_['n_neighbors']}\")\n",
    "# Print R2 score for training set and test set\n",
    "print(f\"R2 score (training): {r2_score(y_trainval, knn_best_normalized_model_trainval_y_pred):.3f}\")\n",
    "print(f\"R2 score (test): {r2_score(y_test, knn_best_normalized_model_test_y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Clustering (20 points) [Sadegh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a dataset consisting of 5 data points $A,B,C,D,E$ as shown in Figure 2. The pair next to each point shows the $x$ and $y$ coordinates of each data point. We wish to group the datapoints into 3 clusters according to the k-means criterion, and using the `k-means++` algorithm.\n",
    "\n",
    "![fig2](images/figure2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'A': np.array([25., 0.]),\n",
    "    'B': np.array([0., 3.]),\n",
    "    'C': np.array([0., -3.]),\n",
    "    'D': np.array([-25., 1.]),\n",
    "    'E': np.array([-25., -1.]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How does the algorithm choose the first initial cluster center $c_1$? (In other words, determine the probability of each data point being chosen as $c_1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conditioned on $C$ being chosen as $c_1$, how does the algorithm choose $c_2$? Report the associated probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "def kmeans_plusplus_initialization(data, initial_centroids=[]):\n",
    "    centroids_indices = initial_centroids.copy()\n",
    "    total_centroids = len(initial_centroids) + 1\n",
    "\n",
    "    for _ in range(total_centroids - len(initial_centroids)):\n",
    "        squared_distances = []\n",
    "        nearest_distances = []\n",
    "\n",
    "        # Distance D(x) for each data point\n",
    "        for point_name in data:\n",
    "            distances_to_centroids = [np.linalg.norm(data[point_name] - data[c]) for c in centroids_indices]\n",
    "            nearest_distance = min(distances_to_centroids)\n",
    "            nearest_distances.append(nearest_distance)\n",
    "            squared_distances.append(nearest_distance ** 2)\n",
    "\n",
    "        # Weighted probability\n",
    "        total_squared_distance = sum(squared_distances)\n",
    "        probabilities = [d / total_squared_distance for d in squared_distances]\n",
    "        \n",
    "        # Print distances and probabilities for the next cluster center\n",
    "        print(f\"\\nDistances and Probabilities for choosing cluster center C{len(centroids_indices) + 1}:\")\n",
    "        table_data = []\n",
    "        for point_name, dist, prob in zip(data.keys(), nearest_distances, probabilities):\n",
    "            table_data.append([point_name, dist, prob])\n",
    "        headers = [\"Point\", \"Distance\", \"Probability\"]\n",
    "        print(tabulate(table_data, headers=headers, floatfmt=(\".3f\", \".3f\", \".3f\")))\n",
    "\n",
    "        # Choose new center with weighted probability\n",
    "        next_centroid = np.random.choice(list(data.keys()), p=probabilities)\n",
    "        centroids_indices.append(next_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distances and Probabilities for choosing cluster center C2:\n",
      "Point      Distance    Probability\n",
      "-------  ----------  -------------\n",
      "A            25.179          0.327\n",
      "B             6.000          0.019\n",
      "C             0.000          0.000\n",
      "D            25.318          0.330\n",
      "E            25.080          0.324\n"
     ]
    }
   ],
   "source": [
    "kmeans_plusplus_initialization(data, ['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conditioned on $c_2 = A$ and $c_1 = C$, how does the algorithm choose $c_3$? Report the associated probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distances and Probabilities for choosing cluster center C3:\n",
      "Point      Distance    Probability\n",
      "-------  ----------  -------------\n",
      "A             0.000          0.000\n",
      "B             6.000          0.028\n",
      "C             0.000          0.000\n",
      "D            25.318          0.491\n",
      "E            25.080          0.482\n"
     ]
    }
   ],
   "source": [
    "kmeans_plusplus_initialization(data, ['C', 'A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Suppose that the algorithm chooses $c_3 = D$ --- hence, $c_1 = C$, $c_2 = A$, $c_3 = D$. Determine the final clustering returned by `k-means++` based on these initial cluster centers (i.e., determine the final clustering and the associated cluster centers). Also argue how you determined that the algorithm has converged. Further, compute the corresponding cost $\\phi_1 := \\phi(\\mathcal{X}, \\mathcal{C})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "def kmeans(data, initial_centroids):\n",
    "    centroids = np.array([data[point] for point in initial_centroids])\n",
    "    prev_centroids = np.zeros(centroids.shape)\n",
    "    clusters = {i: [] for i in range(len(centroids))}\n",
    "    iteration = 1\n",
    "    \n",
    "    # Continue until centroids converge\n",
    "    while not np.array_equal(centroids, prev_centroids):\n",
    "        print(f\"\\nIteration {iteration}:\\n{'-' * 20}\")\n",
    "        clusters = {i: [] for i in range(len(centroids))}\n",
    "\n",
    "        # Assignment step\n",
    "        table_data = []\n",
    "        for point_name, point_coords in data.items():\n",
    "            distances = np.linalg.norm(point_coords - centroids, axis=1)\n",
    "            cluster_index = np.argmin(distances)\n",
    "            clusters[cluster_index].append(point_name)\n",
    "            table_data.append([point_name] + list(distances))\n",
    "        headers = [\"Point\"] + [f\"c{i+1}\" for i in range(len(centroids))]\n",
    "        print(tabulate(table_data, headers=headers, floatfmt=\".3f\"))        \n",
    "\n",
    "        # Update step\n",
    "        prev_centroids = centroids.copy()\n",
    "        for i, point_names in clusters.items():\n",
    "            if point_names:\n",
    "                new_centroid = np.mean([data[pn] for pn in point_names], axis=0)\n",
    "                centroids[i] = new_centroid\n",
    "                print(f\"Updated cluster center, c{i+1}: from [{prev_centroids[i][0]:.3f}, \"\n",
    "                      f\"{prev_centroids[i][1]:.3f}] to [{new_centroid[0]:.3f}, {new_centroid[1]:.3f}]\")\n",
    "\n",
    "        for i, point_names in clusters.items():\n",
    "            print(f\"Cluster c{i+1}: {point_names}\")\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    # Display final results\n",
    "    print(\"\\nFinal Results:\\n\" + '-' * 20)\n",
    "    for i, point_names in clusters.items():\n",
    "        print(f\"Cluster c{i+1}: {point_names}\")\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        formatted_centroid = [f\"{element:.3f}\" for element in centroid]\n",
    "        print(f\"c{i+1} = [{', '.join(formatted_centroid)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1:\n",
      "--------------------\n",
      "Point        c1      c2      c3\n",
      "-------  ------  ------  ------\n",
      "A        25.179   0.000  50.010\n",
      "B         6.000  25.179  25.080\n",
      "C         0.000  25.179  25.318\n",
      "D        25.318  50.010   0.000\n",
      "E        25.080  50.010   2.000\n",
      "Updated cluster center, c1: from [0.000, -3.000] to [0.000, 0.000]\n",
      "Updated cluster center, c2: from [25.000, 0.000] to [25.000, 0.000]\n",
      "Updated cluster center, c3: from [-25.000, 1.000] to [-25.000, 0.000]\n",
      "Cluster c1: ['B', 'C']\n",
      "Cluster c2: ['A']\n",
      "Cluster c3: ['D', 'E']\n",
      "\n",
      "Iteration 2:\n",
      "--------------------\n",
      "Point        c1      c2      c3\n",
      "-------  ------  ------  ------\n",
      "A        25.000   0.000  50.000\n",
      "B         3.000  25.179  25.179\n",
      "C         3.000  25.179  25.179\n",
      "D        25.020  50.010   1.000\n",
      "E        25.020  50.010   1.000\n",
      "Updated cluster center, c1: from [0.000, 0.000] to [0.000, 0.000]\n",
      "Updated cluster center, c2: from [25.000, 0.000] to [25.000, 0.000]\n",
      "Updated cluster center, c3: from [-25.000, 0.000] to [-25.000, 0.000]\n",
      "Cluster c1: ['B', 'C']\n",
      "Cluster c2: ['A']\n",
      "Cluster c3: ['D', 'E']\n",
      "\n",
      "Final Results:\n",
      "--------------------\n",
      "Cluster c1: ['B', 'C']\n",
      "Cluster c2: ['A']\n",
      "Cluster c3: ['D', 'E']\n",
      "c1 = [0.000, 0.000]\n",
      "c2 = [25.000, 0.000]\n",
      "c3 = [-25.000, 0.000]\n"
     ]
    }
   ],
   "source": [
    "kmeans(data, ['C', 'A', 'D'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Now suppose that we had $c_1 = D$, $c_2 = A$, $c_3 = E$. Repeat the previous part with these initial cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1:\n",
      "--------------------\n",
      "Point        c1      c2      c3\n",
      "-------  ------  ------  ------\n",
      "A        50.010   0.000  50.010\n",
      "B        25.080  25.179  25.318\n",
      "C        25.318  25.179  25.080\n",
      "D         0.000  50.010   2.000\n",
      "E         2.000  50.010   0.000\n",
      "Updated cluster center, c1: from [-25.000, 1.000] to [-12.500, 2.000]\n",
      "Updated cluster center, c2: from [25.000, 0.000] to [25.000, 0.000]\n",
      "Updated cluster center, c3: from [-25.000, -1.000] to [-12.500, -2.000]\n",
      "Cluster c1: ['B', 'D']\n",
      "Cluster c2: ['A']\n",
      "Cluster c3: ['C', 'E']\n",
      "\n",
      "Iteration 2:\n",
      "--------------------\n",
      "Point        c1      c2      c3\n",
      "-------  ------  ------  ------\n",
      "A        37.553   0.000  37.553\n",
      "B        12.540  25.179  13.463\n",
      "C        13.463  25.179  12.540\n",
      "D        12.540  50.010  12.855\n",
      "E        12.855  50.010  12.540\n",
      "Updated cluster center, c1: from [-12.500, 2.000] to [-12.500, 2.000]\n",
      "Updated cluster center, c2: from [25.000, 0.000] to [25.000, 0.000]\n",
      "Updated cluster center, c3: from [-12.500, -2.000] to [-12.500, -2.000]\n",
      "Cluster c1: ['B', 'D']\n",
      "Cluster c2: ['A']\n",
      "Cluster c3: ['C', 'E']\n",
      "\n",
      "Final Results:\n",
      "--------------------\n",
      "Cluster c1: ['B', 'D']\n",
      "Cluster c2: ['A']\n",
      "Cluster c3: ['C', 'E']\n",
      "c1 = [-12.500, 2.000]\n",
      "c2 = [25.000, 0.000]\n",
      "c3 = [-12.500, -2.000]\n"
     ]
    }
   ],
   "source": [
    "kmeans(data, ['D', 'A', 'E'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Letting $\\phi'$ denote the cost in the previous part, what is the approximation ratio $\\phi'/\\phi^\\star$. (In this example, $\\phi^\\starϕ = 20$). Explain whether or not your calculated ratio agrees with the theoretical guarantee of `k-means++` (see slides)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
